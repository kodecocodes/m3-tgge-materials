{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python with the Gemini API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must make a .env file with your API Key in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the google gemini api and the dotenv library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q google-generativeai\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbailey/Desktop/m3-tgge-materials/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ.get('GOOGLE_API_KEY')\n",
    "\n",
    "# Configuring the API key.\n",
    "genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723437144.493199 1220111 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerateContent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_generation_methods\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/m3-tgge-materials/.venv/lib/python3.12/site-packages/google/generativeai/models.py:204\u001b[0m, in \u001b[0;36mlist_models\u001b[0;34m(page_size, client, request_options)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_default_model_client()\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    205\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39mto_dict(model)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_types\u001b[38;5;241m.\u001b[39mModel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel)\n",
      "File \u001b[0;32m~/Desktop/m3-tgge-materials/.venv/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/model_service/client.py:910\u001b[0m, in \u001b[0;36mModelServiceClient.list_models\u001b[0;34m(self, request, page_size, page_token, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    909\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 910\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m# This method is paged; wrap the response in a pager, which provides\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;66;03m# an `__iter__` convenience method.\u001b[39;00m\n\u001b[1;32m    919\u001b[0m response \u001b[38;5;241m=\u001b[39m pagers\u001b[38;5;241m.\u001b[39mListModelsPager(\n\u001b[1;32m    920\u001b[0m     method\u001b[38;5;241m=\u001b[39mrpc,\n\u001b[1;32m    921\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[1;32m    922\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    923\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    924\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/m3-tgge-materials/.venv/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/m3-tgge-materials/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/m3-tgge-materials/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/Desktop/m3-tgge-materials/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/Desktop/m3-tgge-materials/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/Desktop/m3-tgge-materials/.venv/lib/python3.12/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/m3-tgge-materials/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n]"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the details about a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name='models/gemini-pro-vision',\n",
       "      base_model_id='',\n",
       "      version='001',\n",
       "      display_name='Gemini 1.0 Pro Vision',\n",
       "      description='The best image understanding model to handle a broad range of applications',\n",
       "      input_token_limit=12288,\n",
       "      output_token_limit=4096,\n",
       "      supported_generation_methods=['generateContent', 'countTokens'],\n",
       "      temperature=0.4,\n",
       "      max_temperature=None,\n",
       "      top_p=1.0,\n",
       "      top_k=32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genai.get_model('models/gemini-pro-vision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GenerativeModel instance using the Gemini-Pro model\n",
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt for a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('What kind of safety features does Google Gemini API provide for prompts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Google Gemini API provides a range of safety features for prompts to help protect users from harmful or inappropriate content. These features include:\\n\\n* **Profanity filtering:** The API automatically filters out profane words and phrases from prompts.\\n* **Hate speech detection:** The API uses machine learning models to detect and flag prompts that contain hate speech.\\n* **Violence detection:** The API uses machine learning models to detect and flag prompts that contain violent content.\\n* **Sexual content detection:** The API uses machine learning models to detect and flag prompts that contain sexual content.\\n* **Policy compliance:** The API ensures that prompts comply with Google's content policies.\\n\\nIn addition to these safety features, the Gemini API also provides a number of tools and resources to help developers create safe and responsible applications. These resources include:\\n\\n* **Documentation on safety best practices:** The Gemini API documentation provides detailed guidance on how to create safe and responsible applications.\\n* **Developer support:** The Gemini API team is available to provide support and guidance to developers on how to use the API safely and responsibly.\\n\\nBy using the safety features and resources provided by the Gemini API, developers can help to create safe and responsible applications that protect users from harmful or inappropriate content.\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 13,\n",
       "        \"candidates_token_count\": 250,\n",
       "        \"total_token_count\": 263\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the test of the response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Gemini API provides a range of safety features for prompts to help protect users from harmful or inappropriate content. These features include:\n",
      "\n",
      "* **Profanity filtering:** The API automatically filters out profane words and phrases from prompts.\n",
      "* **Hate speech detection:** The API uses machine learning models to detect and flag prompts that contain hate speech.\n",
      "* **Violence detection:** The API uses machine learning models to detect and flag prompts that contain violent content.\n",
      "* **Sexual content detection:** The API uses machine learning models to detect and flag prompts that contain sexual content.\n",
      "* **Policy compliance:** The API ensures that prompts comply with Google's content policies.\n",
      "\n",
      "In addition to these safety features, the Gemini API also provides a number of tools and resources to help developers create safe and responsible applications. These resources include:\n",
      "\n",
      "* **Documentation on safety best practices:** The Gemini API documentation provides detailed guidance on how to create safe and responsible applications.\n",
      "* **Developer support:** The Gemini API team is available to provide support and guidance to developers on how to use the API safely and responsibly.\n",
      "\n",
      "By using the safety features and resources provided by the Gemini API, developers can help to create safe and responsible applications that protect users from harmful or inappropriate content.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the safety feedback for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response.prompt_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt with a questionably unsafe prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('List some prompts that are flagged as hate speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"finish_reason\": \"SAFETY\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"LOW\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"HIGH\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"HIGH\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"MEDIUM\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 10,\n",
       "        \"total_token_count\": 10\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Gemini API provides a range of safety features for prompts to help protect users from harmful or inappropriate content. These features include:\n",
      "\n",
      "* **Profanity filtering:** The API automatically filters out profane words and phrases from prompts.\n",
      "* **Hate speech detection:** The API uses machine learning models to detect and flag prompts that contain hate speech.\n",
      "* **Violence detection:** The API uses machine learning models to detect and flag prompts that contain violent content.\n",
      "* **Sexual content detection:** The API uses machine learning models to detect and flag prompts that contain sexual content.\n",
      "* **Policy compliance:** The API ensures that prompts comply with Google's content policies.\n",
      "\n",
      "In addition to these safety features, the Gemini API also provides a number of tools and resources to help developers create safe and responsible applications. These resources include:\n",
      "\n",
      "* **Documentation on safety best practices:** The Gemini API documentation provides detailed guidance on how to create safe and responsible applications.\n",
      "* **Developer support:** The Gemini API team is available to provide support and guidance to developers on how to use the API safely and responsibly.\n",
      "\n",
      "By using the safety features and resources provided by the Gemini API, developers can help to create safe and responsible applications that protect users from harmful or inappropriate content.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the safety feedback for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.prompt_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt with a questionably unsafe prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('I\\'m going to slap you into next week!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Please refrain from using violent language. It is not appropriate to threaten someone with violence.\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 12,\n",
       "        \"candidates_token_count\": 17,\n",
       "        \"total_token_count\": 29\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please refrain from using violent language. It is not appropriate to threaten someone with violence.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import safety settings libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust safety settings... You can try different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"finish_reason\": \"SAFETY\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"MEDIUM\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"MEDIUM\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 11,\n",
      "        \"total_token_count\": 11\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "safety = {\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    }\n",
    "response = model.generate_content(\n",
    "    'All purple people eaters are homicidal maniacs!!!',\n",
    "    safety_settings= safety\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a generation_config and try different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = genai.types.GenerationConfig(\n",
    "    candidate_count=1,\n",
    "    # stop_sequences=[';']\n",
    "    # max_output_tokens=32000\n",
    "    # temperature=0.0,\n",
    "    # top_k=4,\n",
    "    # top_p=1\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the config with a prompt and try different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('Write a rap about doing the dishes',generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the settings model-wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro', generation_config=generation_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
