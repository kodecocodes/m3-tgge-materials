{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python with the Gemini API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must make a .env file with your API Key in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the google gemini api and the dotenv library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q google-generativeai\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "os.environ.get('GOOGLE_API_KEY')\n",
    "\n",
    "# Configuring the API key.\n",
    "genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723065868.584716 2135389 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001-tuning\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the details about a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name='models/gemini-pro-vision',\n",
       "      base_model_id='',\n",
       "      version='001',\n",
       "      display_name='Gemini 1.0 Pro Vision',\n",
       "      description='The best image understanding model to handle a broad range of applications',\n",
       "      input_token_limit=12288,\n",
       "      output_token_limit=4096,\n",
       "      supported_generation_methods=['generateContent', 'countTokens'],\n",
       "      temperature=0.4,\n",
       "      max_temperature=None,\n",
       "      top_p=1.0,\n",
       "      top_k=32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genai.get_model('models/gemini-pro-vision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GenerativeModel instance using the Gemini-Pro model\n",
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt for a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('What kind of safety features does Google Gemini API provide for prompts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Google Gemini API provides a range of safety features for prompts to help protect users from harmful or inappropriate content. These features include:\\n\\n* **Profanity filtering:** The API automatically filters out profane words and phrases from prompts.\\n* **Hate speech detection:** The API uses machine learning models to detect and flag prompts that contain hate speech.\\n* **Violence detection:** The API uses machine learning models to detect and flag prompts that contain violent content.\\n* **Sexual content detection:** The API uses machine learning models to detect and flag prompts that contain sexual content.\\n* **Policy compliance:** The API ensures that prompts comply with Google's content policies.\\n\\nIn addition to these safety features, the Gemini API also provides a number of tools and resources to help developers create safe and responsible applications. These resources include:\\n\\n* **Documentation on safety best practices:** The Gemini API documentation provides detailed guidance on how to create safe and responsible applications.\\n* **Developer support:** The Gemini API team is available to provide support and guidance to developers on how to use the API safely and responsibly.\\n\\nBy using the safety features and resources provided by the Gemini API, developers can help to create safe and responsible applications that protect users from harmful or inappropriate content.\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 13,\n",
       "        \"candidates_token_count\": 250,\n",
       "        \"total_token_count\": 263\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the test of the response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Gemini API provides a range of safety features for prompts to help protect users from harmful or inappropriate content. These features include:\n",
      "\n",
      "* **Profanity filtering:** The API automatically filters out profane words and phrases from prompts.\n",
      "* **Hate speech detection:** The API uses machine learning models to detect and flag prompts that contain hate speech.\n",
      "* **Violence detection:** The API uses machine learning models to detect and flag prompts that contain violent content.\n",
      "* **Sexual content detection:** The API uses machine learning models to detect and flag prompts that contain sexual content.\n",
      "* **Policy compliance:** The API ensures that prompts comply with Google's content policies.\n",
      "\n",
      "In addition to these safety features, the Gemini API also provides a number of tools and resources to help developers create safe and responsible applications. These resources include:\n",
      "\n",
      "* **Documentation on safety best practices:** The Gemini API documentation provides detailed guidance on how to create safe and responsible applications.\n",
      "* **Developer support:** The Gemini API team is available to provide support and guidance to developers on how to use the API safely and responsibly.\n",
      "\n",
      "By using the safety features and resources provided by the Gemini API, developers can help to create safe and responsible applications that protect users from harmful or inappropriate content.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the safety feedback for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response.prompt_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt with a questionably unsafe prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('List some prompts that are flagged as hate speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"finish_reason\": \"SAFETY\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"LOW\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"HIGH\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"HIGH\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"MEDIUM\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 10,\n",
       "        \"total_token_count\": 10\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Gemini API provides a range of safety features for prompts to help protect users from harmful or inappropriate content. These features include:\n",
      "\n",
      "* **Profanity filtering:** The API automatically filters out profane words and phrases from prompts.\n",
      "* **Hate speech detection:** The API uses machine learning models to detect and flag prompts that contain hate speech.\n",
      "* **Violence detection:** The API uses machine learning models to detect and flag prompts that contain violent content.\n",
      "* **Sexual content detection:** The API uses machine learning models to detect and flag prompts that contain sexual content.\n",
      "* **Policy compliance:** The API ensures that prompts comply with Google's content policies.\n",
      "\n",
      "In addition to these safety features, the Gemini API also provides a number of tools and resources to help developers create safe and responsible applications. These resources include:\n",
      "\n",
      "* **Documentation on safety best practices:** The Gemini API documentation provides detailed guidance on how to create safe and responsible applications.\n",
      "* **Developer support:** The Gemini API team is available to provide support and guidance to developers on how to use the API safely and responsibly.\n",
      "\n",
      "By using the safety features and resources provided by the Gemini API, developers can help to create safe and responsible applications that protect users from harmful or inappropriate content.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the safety feedback for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.prompt_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt with a questionably unsafe prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('I\\'m going to slap you into next week!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Please refrain from using violent language. It is not appropriate to threaten someone with violence.\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 12,\n",
       "        \"candidates_token_count\": 17,\n",
       "        \"total_token_count\": 29\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please refrain from using violent language. It is not appropriate to threaten someone with violence.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import safety settings libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust safety settings... You can try different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"finish_reason\": \"SAFETY\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"MEDIUM\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"MEDIUM\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 11,\n",
      "        \"total_token_count\": 11\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "safety = {\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    }\n",
    "response = model.generate_content(\n",
    "    'All purple people eaters are homicidle maniacs!!!',\n",
    "    safety_settings= safety\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a generation_config and try different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = genai.types.GenerationConfig(\n",
    "    candidate_count=1,\n",
    "    # stop_sequences=[';']\n",
    "    # max_output_tokens=32000\n",
    "    # temperature=0.0,\n",
    "    # top_k=4,\n",
    "    # top_p=1\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the config with a prompt and try different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('Write a rap about doing the dishes',generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the settings model-wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro', generation_config=generation_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
